{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMctw4I8wmQpL9tNMmvMwW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasgab2230/LuwiAI-V2-Coder/blob/main/src/data_downloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ncHe_aqY3nQ"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a code that search the technologies docs URLs and they sub-URLs and download code snippets from URLs. However, it will use BeautifulSoup and Requests to search across the internet.\n",
        "\n",
        "!pip install requests beautifulsoup4\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def find_code_snippets(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10) # Added timeout for robustness\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        code_snippets = []\n",
        "        # This is a very basic search for <pre> and <code> tags.\n",
        "        # You would need much more sophisticated logic here.\n",
        "        for pre_tag in soup.find_all('pre'):\n",
        "            code_snippets.append(pre_tag.get_text())\n",
        "        for code_tag in soup.find_all('code'):\n",
        "             code_snippets.append(code_tag.get_text())\n",
        "\n",
        "        return code_snippets\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred processing {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Example usage (replace with actual tech documentation URLs)\n",
        "# You would need to expand this to handle multiple URLs and crawl links.\n",
        "initial_urls = [\n",
        "    'https://docs.python.org/3/',\n",
        "    'https://developer.mozilla.org/en-US/docs/Web/HTML', # HTML Docs\n",
        "    'https://developer.mozilla.org/en-US/docs/Web/CSS',  # CSS Docs\n",
        "    'https://developer.mozilla.org/en-US/docs/Web/JavaScript', # JS Docs\n",
        "    'https://react.dev/reference/react', # React Docs\n",
        "    'https://git-scm.com/doc', # Git Docs\n",
        "    'https://www.typescriptlang.org/docs/' # TS Docs\n",
        "    # Add more initial documentation URLs here\n",
        "]\n",
        "\n",
        "all_snippets = {}\n",
        "visited_urls = set()\n",
        "url_queue = initial_urls[:]\n",
        "\n",
        "while url_queue:\n",
        "    current_url = url_queue.pop(0)\n",
        "    if current_url in visited_urls:\n",
        "        continue\n",
        "\n",
        "    print(f\"Visiting: {current_url}\")\n",
        "    visited_urls.add(current_url)\n",
        "\n",
        "    snippets = find_code_snippets(current_url)\n",
        "    if snippets:\n",
        "        all_snippets[current_url] = snippets\n",
        "        # In a real application, you would save these snippets to files or a database\n",
        "\n",
        "    # --- Basic link finding (needs significant improvement for real-world scraping) ---\n",
        "    try:\n",
        "        response = requests.get(current_url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            # Very basic check: only process absolute URLs for now and stay on the same domain\n",
        "            if href.startswith('http') and requests.utils.urlparse(href).netloc == requests.utils.urlparse(current_url).netloc:\n",
        "                 if href not in visited_urls and href not in url_queue:\n",
        "                    url_queue.append(href)\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding links on {current_url}: {e}\")\n",
        "    # --- End of basic link finding ---\n",
        "\n",
        "    # Add a small delay to be polite to the server\n",
        "    time.sleep(1)\n",
        "\n",
        "# Print some of the collected snippets\n",
        "for url, snippets in list(all_snippets.items())[:5]: # Print snippets from the first 5 URLs\n",
        "    print(f\"\\nCode Snippets from {url}:\")\n",
        "    for i, snippet in enumerate(snippets):\n",
        "        print(f\"Snippet {i+1}:\\n{snippet}\\n---\")"
      ]
    }
  ]
}